{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN Tutorial\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contents\n",
    "1. Introduction to RNN\n",
    "2. RNN Cell Structure\n",
    "3. Different types of RNN\n",
    "4. Sentiment Analysis via RNN\n",
    "5. Text Generation via LSTM RNN\n",
    "6. References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Text Generation via LSTM RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing the dependencies\n",
    "import sys\n",
    "import numpy\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import np_utils\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load ascii text and conver to lowercase\n",
    "filename = \"data.txt\"\n",
    "raw_text = open(filename).read()\n",
    "raw_text = raw_text.lower()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract all the unique characters from the text\n",
    "chars = sorted(list(set(raw_text)))\n",
    "char_to_int  = dict((c,i) for i,c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets Have a look at some data in our character list\n",
    "chars[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Summary of the dataset\n",
    "n_chars = len(raw_text)\n",
    "n_vocab = len(chars)\n",
    "print(f'Total characters: {n_chars}')\n",
    "print(f'Total Vocab: {n_vocab}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Now is the time to define the training mechanism of the network. \n",
    "2. There can be many ways to split the data into chunks of a fixed size which can be fed to the network.\n",
    "3. The target would be the character next to the chunk end.\n",
    "\n",
    "4. we will split the book text up into subsequences with a fixed length of 100 characters, an arbitrary length. \n",
    "5. We could just as easily split the data up by sentences and pad the shorter sequences and truncate the longer ones.\n",
    "6. Each training pattern of the network is comprised of 100 time steps of one character (X) followed by one character output (y)\n",
    "7. When creating these sequences, we slide this window along the whole book one character at a time allowing each character a chance to be learned from the 100 characters that preceded it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encode the dataset of input to output pairs as integers\n",
    "seq_length=100\n",
    "dataX = []\n",
    "dataY = []\n",
    "\n",
    "for i in range(0,n_chars-seq_length,1):\n",
    "    seq_in = raw_text[i:i+seq_length]\n",
    "    seq_out = raw_text[i+seq_length]\n",
    "    dataX.append([char_to_int[char] for char in seq_in])\n",
    "    dataY.append(char_to_int[seq_out])\n",
    "n_patterns = len(dataX)\n",
    "print(f'Total patterns: {n_patterns}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now let's transform the list of input sequences into the form [samples, time steps, features] expected by an LSTM network.\n",
    "X = numpy.reshape(dataX,(n_patterns,seq_length,1))\n",
    "#normalize the data to be easily fed to the LSTM\n",
    "X = X/float(n_vocab)\n",
    "#One hot encode the output variable\n",
    "y = np_utils.to_categorical(dataY)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print out the Shape of X\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print out the Shape of y\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the LSTM Model using Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Specify the model \n",
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(X.shape[1],X.shape[2])))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(y.shape[1],activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy',optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#As the model above takes time to train, we will use model checkpoint feature of keras  to save the training weights periodically \n",
    "filepath=\"weights-improvement-{epoch:02d}-{loss:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit the model \n",
    "model.fit(X, y, epochs=20, batch_size=128, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating the Text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the saved Weights\n",
    "\n",
    "filename = \"weights-improvement-epoch-loss.hdf5\"\n",
    "model.load_weights(filename)\n",
    "model.compile(loss=\"categorical_crossentropy\",optimizer='adam')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reverse encode the integers to text\n",
    "int_to_char = dict((i, c) for i, c in enumerate(chars))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pick a random seed\n",
    "start = numpy.random.randint(0,len(dataX)-1)\n",
    "pattern = dataX[start]\n",
    "print(f\"Seed is \\n {''.join([int_to_char[value] for value in pattern])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate the text\n",
    "for i in range(400):\n",
    "    x  = numpy.reshape(pattern,(1,len(pattern),1))\n",
    "    x  = x/float(n_vocab)\n",
    "    prediction = model.predict(x,verbose=0)\n",
    "    index  = numpy.argmax(prediction)\n",
    "    result = int_to_char[index]\n",
    "    seq_in = [int_to_char[value] for value in pattern]\n",
    "    sys.stdout.write(result)\n",
    "    pattern.append(index)\n",
    "    pattern = pattern[1:len(pattern)]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN Tutorial\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contents\n",
    "1. <big>What is a RNN</big>\n",
    "2. <big> What can they do?</big>\n",
    "3. <big> Vanishing Gradient </big>\n",
    "4. <big> LSTMs </big>\n",
    "5. <big>Text Generation via LSTM RNN</big>\n",
    "6. <big>Resources</big>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is a RNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<big>Recurrent Neural Networks take the previous output or hidden states as inputs.</big>\n",
    "<img src=\"images/rnn.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why RNNs?\n",
    "<big>Because not all problems can be converted into one with fixedlength inputs and outputs. <br></big>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What can they do?\n",
    "1. <big> Image Captioning </big>\n",
    "2. <big> Machine  Translation </big>\n",
    "3. <big>Sentiment Classification </big>\n",
    "4. <big> Time series Prediction </big>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Types of Inputs to an RNN\n",
    "<img src=\"images/whyrnns.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<big>Now, even though RNNs are quite powerful\n",
    "they suffer from **Vanishing gradient** problem  which hinders them from using long term information \n",
    "like they are good for storing memory 3-4 instances of past iterations \n",
    "but larger number of instances don't provide good results so we don't just use regular RNNs. </big>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vanishing Gradient in RNNs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/vangrad.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<big>Instead, we use a better variation of RNNs: **Long Short Term Networks(LSTM).** <big>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.Long Short Term Memory(LSTM)\n",
    "\n",
    "Long short-term memory (LSTM) units (or blocks) are a building unit for layers of a recurrent neural network (RNN). \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"images/lstm_chain.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<big>The expression long short-term refers to the fact that LSTM is a model for the short-term memory which can last for a long period of time. </big>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Components of LSTMs\n",
    "So the LSTM cell contains the following components\n",
    "* <big>Input Gate </big>\n",
    "* <big> Learn Gate</big> \n",
    "* <big> Remember Gate</big>\n",
    "* <big> Forget Gate </big>\n",
    "* <big> Output Gate</big>\n",
    "* <big> Use Gate</big>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Text Generation via LSTM RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing the dependencies\n",
    "import sys\n",
    "import numpy\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import np_utils\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load ascii text and conver to lowercase\n",
    "filename = \"data.txt\"\n",
    "raw_text = open(filename).read()\n",
    "raw_text = raw_text.lower()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract all the unique characters from the text\n",
    "chars = sorted(list(set(raw_text)))\n",
    "char_to_int  = dict((c,i) for i,c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets Have a look at some data in our character list\n",
    "chars[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Summary of the dataset\n",
    "n_chars = len(raw_text)\n",
    "n_vocab = len(chars)\n",
    "print(f'Total characters: {n_chars}')\n",
    "print(f'Total Vocab: {n_vocab}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<big>\n",
    "1. Now is the time to define the training mechanism of the network. <br>\n",
    "2. There can be many ways to split the data into chunks of a fixed size which can be fed to the network.<br>\n",
    "3. The target would be the character next to the chunk end.<br>\n",
    "4. we will split the book text up into subsequences with a fixed length of 100 characters, an arbitrary length. <br>\n",
    "5. We could just as easily split the data up by sentences and pad the shorter sequences and truncate the longer ones.<br>\n",
    "6. Each training pattern of the network is comprised of 100 time steps of one character (X) followed by one character output (y)<br>\n",
    "7. When creating these sequences, we slide this window along the whole book one character at a time allowing each character a chance to be learned from the 100 characters that preceded it. </big>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encode the dataset of input to output pairs as integers\n",
    "seq_length=100\n",
    "dataX = []\n",
    "dataY = []\n",
    "\n",
    "for i in range(0,n_chars-seq_length,1):\n",
    "    seq_in = raw_text[i:i+seq_length]\n",
    "    seq_out = raw_text[i+seq_length]\n",
    "    dataX.append([char_to_int[char] for char in seq_in])\n",
    "    dataY.append(char_to_int[seq_out])\n",
    "n_patterns = len(dataX)\n",
    "print(f'Total patterns: {n_patterns}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now let's transform the list of input sequences into the form [samples, time steps, features] expected by an LSTM network.\n",
    "X = numpy.reshape(dataX,(n_patterns,seq_length,1))\n",
    "#normalize the data to be easily fed to the LSTM\n",
    "X = X/float(n_vocab)\n",
    "#One hot encode the output variable\n",
    "y = np_utils.to_categorical(dataY)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print out the Shape of X\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print out the Shape of y\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the LSTM Model using Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Specify the model \n",
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(X.shape[1],X.shape[2])))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(y.shape[1],activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy',optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#As the model above takes time to train, we will use model checkpoint feature of keras  to save the training weights periodically \n",
    "filepath=\"weights-improvement-{epoch:02d}-{loss:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit the model \n",
    "model.fit(X, y, epochs=20, batch_size=128, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating the Text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"weights-improvement-epoch-loss.hdf5\"\n",
    "model.load_weights(filename)\n",
    "model.compile(loss=\"categorical_crossentropy\",optimizer='adam')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reverse encode the integers to text\n",
    "int_to_char = dict((i, c) for i, c in enumerate(chars))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pick a random seed\n",
    "start = numpy.random.randint(0,len(dataX)-1)\n",
    "pattern = dataX[start]\n",
    "print(f\"Seed is \\n {''.join([int_to_char[value] for value in pattern])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate the text\n",
    "for i in range(400):\n",
    "    x  = numpy.reshape(pattern,(1,len(pattern),1))\n",
    "    x  = x/float(n_vocab)\n",
    "    prediction = model.predict(x,verbose=0)\n",
    "    index  = numpy.argmax(prediction)\n",
    "    result = int_to_char[index]\n",
    "    seq_in = [int_to_char[value] for value in pattern]\n",
    "    sys.stdout.write(result)\n",
    "    pattern.append(index)\n",
    "    pattern = pattern[1:len(pattern)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Resources\n",
    "\n",
    "\n",
    "1. [Understanding LSTM Networks-Colah's Blog](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)\n",
    "2. [The Unreasonable Effectiveness of RNNs](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)\n",
    "3. [Bi-LSTM with Pytorch](https://www.kaggle.com/ziliwang/baseline-pytorch-bilstm)\n",
    "4. [A friendly introduction to Recurrent Neural Networks](https://www.youtube.com/watch?v=UNmqTiOnRfg&t=3s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
